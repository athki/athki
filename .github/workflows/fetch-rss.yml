name: Fetch Substack RSS Feed

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  fetch-rss:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install requests
        
    - name: Fetch RSS feed
      run: |
        python -c "
        import requests
        import json
        import os
        from datetime import datetime
        
        # Replace with your actual Substack name
        SUBSTACK_NAME = 'computingpower'
        RSS_URL = f'https://{SUBSTACK_NAME}.substack.com/feed'
        
        print(f'Fetching RSS feed from: {RSS_URL}')
        
        try:
            response = requests.get(RSS_URL, timeout=30)
            print(f'Response status: {response.status_code}')
            response.raise_for_status()
            
            # Parse XML and extract posts
            import xml.etree.ElementTree as ET
            
            # Clean up the XML content (remove newlines and extra spaces)
            xml_content = response.content.decode('utf-8').replace('\n', '').replace('\r', '')
            print(f'XML content length: {len(xml_content)}')
            print(f'First 500 chars: {xml_content[:500]}')
            root = ET.fromstring(xml_content)
            
            print(f'Root tag: {root.tag}')
            channel = root.find("channel")
            if channel:
                print(f'Channel tag: {channel.tag}')
            else:
                print('No channel found')
            
            posts = []
            items = root.findall('.//item')
            print(f'Found {len(items)} items in RSS feed')
            
            # Also try alternative selectors
            if len(items) == 0:
                items = root.findall('item')
                print(f'Found {len(items)} items with alternative selector')
            if len(items) == 0:
                channel = root.find('channel')
                if channel:
                    items = channel.findall('item')
                    print(f'Found {len(items)} items in channel')
            
            # Debug: print all tags in the XML
            all_tags = [elem.tag for elem in root.iter()]
            print(f'All tags found: {all_tags}')
            
            # Count items found
            print(f'Total items found: {len(items)}')
            
            for i, item in enumerate(items[:5]):  # Get latest 5 posts
                print(f'Processing item {i+1}:')
                print(f'  Item tag: {item.tag}')
                print(f'  Item children: {[child.tag for child in item]}')
                title_elem = item.find('title')
                link_elem = item.find('link')
                pub_date_elem = item.find('pubDate')
                description_elem = item.find('description')
                
                title = title_elem.text if title_elem is not None else 'Untitled'
                link = link_elem.text if link_elem is not None else '#'
                pub_date = pub_date_elem.text if pub_date_elem is not None else ''
                description = description_elem.text if description_elem is not None else ''
                
                # Clean up CDATA sections
                title = title.replace('<![CDATA[', '').replace(']]>', '')
                description = description.replace('<![CDATA[', '').replace(']]>', '')
                
                print(f'Processing post: {title}')
                
                posts.append({
                    'title': title,
                    'link': link,
                    'pubDate': pub_date,
                    'description': description
                })
            
            # Create data directory if it doesn't exist
            os.makedirs('data', exist_ok=True)
            
            # Save to JSON file
            with open('data/substack-posts.json', 'w') as f:
                json.dump({
                    'lastUpdated': datetime.now().isoformat(),
                    'posts': posts
                }, f, indent=2)
                
            print(f'Successfully fetched {len(posts)} posts')
            
        except Exception as e:
            print(f'Error fetching RSS feed: {e}')
            import traceback
            traceback.print_exc()
            # Create empty file if fetch fails
            os.makedirs('data', exist_ok=True)
            with open('data/substack-posts.json', 'w') as f:
                json.dump({
                    'lastUpdated': datetime.now().isoformat(),
                    'posts': []
                }, f, indent=2)
        "
        
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add data/substack-posts.json
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "Update Substack posts"
          git push
        fi 